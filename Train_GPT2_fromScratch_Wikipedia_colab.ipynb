{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets\n",
        "#%pip install transformers datasets torch\n"
      ],
      "metadata": {
        "id": "tx_c80ybpvqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7i9e-MGpkfH"
      },
      "outputs": [],
      "source": [
        "# 1. **Prepare the Wikipedia dataset**: You can use the `datasets` library to load and preprocess the Wikipedia dataset.\n",
        "# 2. **Define the GPT-2 model**: Modify the existing GPT-2 model to initialize weights from scratch.\n",
        "# 3. **Train the model**: Write a training loop to train the model on the Wikipedia dataset.\n",
        "\n",
        "# Here's the modified code to achieve this:\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import GPT2PreTrainedModel, GPT2Config\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions)\n",
        "from transformers.modeling_utils import Conv1D\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from typing import Tuple\n",
        "\n",
        "class GPT2MLP(nn.Module):\n",
        "    def __init__(self, intermediate_size, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.hidden_size\n",
        "        self.c_fc = Conv1D(intermediate_size, embed_dim)\n",
        "        self.c_proj = Conv1D(embed_dim, intermediate_size)\n",
        "        self.act = ACT2FN[config.activation_function]\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.c_fc(hidden_states)\n",
        "        hidden_states = self.act(hidden_states)\n",
        "        hidden_states = self.c_proj(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class GPT2Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        hidden_size = config.hidden_size\n",
        "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
        "        self.attn = GPT2Attention(config)\n",
        "        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        if config.add_cross_attention:\n",
        "            self.crossattention = GPT2Attention(config, is_cross_attention=True)\n",
        "            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self.mlp = GPT2MLP(inner_dim, config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        layer_past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_1(hidden_states)\n",
        "        attn_outputs = self.attn(\n",
        "            hidden_states,\n",
        "            layer_past=layer_past,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
        "        outputs = attn_outputs[1:]\n",
        "        # residual connection\n",
        "        hidden_states = attn_output + residual\n",
        "\n",
        "        if encoder_hidden_states is not None:\n",
        "            # add one self-attention block for cross-attention\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
        "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "            residual = hidden_states\n",
        "            hidden_states = self.ln_cross_attn(hidden_states)\n",
        "            cross_attn_outputs = self.crossattention(\n",
        "                hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            attn_output = cross_attn_outputs[0]\n",
        "            # residual connection\n",
        "            hidden_states = residual + attn_output\n",
        "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_2(hidden_states)\n",
        "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
        "        # residual connection\n",
        "        hidden_states = residual + feed_forward_hidden_states\n",
        "\n",
        "        if use_cache:\n",
        "            outputs = (hidden_states,) + outputs\n",
        "        else:\n",
        "            outputs = (hidden_states,) + outputs[1:]\n",
        "\n",
        "        return outputs  # hidden_states, present, (attentions, cross_attentions)\n",
        "\n",
        "\n",
        "\n",
        "class GPT2Model(GPT2PreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [\"attn.masked_bias\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.embed_dim = config.hidden_size\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
        "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
        "\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.h = nn.ModuleList([GPT2Block(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.wte\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.wte = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "            batch_size = input_ids.shape[0]\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "            batch_size = inputs_embeds.shape[0]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
        "        if position_ids is not None:\n",
        "            position_ids = position_ids.view(-1, input_shape[-1])\n",
        "\n",
        "        if past_key_values is None:\n",
        "            past_length = 0\n",
        "            past_key_values = tuple([None] * len(self.h))\n",
        "        else:\n",
        "            past_length = past_key_values[0][0].size(-2)\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
        "            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
        "\n",
        "        # GPT2Attention mask.\n",
        "        if attention_mask is not None:\n",
        "            assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
        "            attention_mask = attention_mask.view(batch_size, -1)\n",
        "            attention_mask = attention_mask[:, None, None, :]\n",
        "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
        "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
        "\n",
        "        # If a 2D ou 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.wte(input_ids)\n",
        "        position_embeds = self.wpe(position_ids)\n",
        "        hidden_states = inputs_embeds + position_embeds\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_embeds = self.wte(token_type_ids)\n",
        "            hidden_states = hidden_states + token_type_embeds\n",
        "\n",
        "        hidden_states = self.drop(hidden_states)\n",
        "\n",
        "        output_shape = input_shape + (hidden_states.size(-1),)\n",
        "\n",
        "        presents = () if use_cache else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            outputs = block(\n",
        "                hidden_states,\n",
        "                layer_past=layer_past,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask[i],\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "                use_cache=use_cache,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "\n",
        "            hidden_states = outputs[0]\n",
        "            if use_cache is True:\n",
        "                presents = presents + (outputs[1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
        "\n",
        "\n",
        "        hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.view(*output_shape)\n",
        "        # Add last hidden state\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
        "\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=presents,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class GPT2Model(GPT2PreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [\"attn.masked_bias\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.embed_dim = config.hidden_size\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
        "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
        "\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        self.h = nn.ModuleList([GPT2Block(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights from scratch\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.wte\n",
        "\n",
        "def set_input_embeddings(self, new_embeddings):\n",
        "    self.wte = new_embeddings\n",
        "\n",
        "def forward(\n",
        "    self,\n",
        "    input_ids=None,\n",
        "    past_key_values=None,\n",
        "    attention_mask=None,\n",
        "    token_type_ids=None,\n",
        "    position_ids=None,\n",
        "    head_mask=None,\n",
        "    inputs_embeds=None,\n",
        "    encoder_hidden_states=None,\n",
        "    encoder_attention_mask=None,\n",
        "    use_cache=None,\n",
        "    output_attentions=None,\n",
        "    output_hidden_states=None,\n",
        "    return_dict=None,\n",
        "):\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = (\n",
        "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    )\n",
        "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        batch_size = input_ids.shape[0]\n",
        "    elif inputs_embeds is not None:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        batch_size = inputs_embeds.shape[0]\n",
        "    else:\n",
        "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
        "    if position_ids is not None:\n",
        "        position_ids = position_ids.view(-1, input_shape[-1])\n",
        "\n",
        "    if past_key_values is None:\n",
        "        past_length = 0\n",
        "        past_key_values = tuple([None] * len(self.h))\n",
        "    else:\n",
        "        past_length = past_key_values[0][0].size(-2)\n",
        "    if position_ids is None:\n",
        "        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
        "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
        "\n",
        "    # GPT2Attention mask.\n",
        "    if attention_mask is not None:\n",
        "        assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
        "        attention_mask = attention_mask.view(batch_size, -1)\n",
        "        attention_mask = attention_mask[:, None, None, :]\n",
        "        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
        "        attention_mask = (1.0 - attention_mask) * -10000.0\n",
        "\n",
        "    # If a 2D ou 3D attention mask is provided for the cross-attention\n",
        "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "    if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
        "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "        if encoder_attention_mask is None:\n",
        "            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "        encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "    else:\n",
        "        encoder_attention_mask = None\n",
        "\n",
        "    # Prepare head mask if needed\n",
        "    # 1.0 in head_mask indicate we keep the head\n",
        "    # attention_probs has shape bsz x n_heads x N x N\n",
        "    # head_mask has shape n_layer x batch x n_heads x N x N\n",
        "    head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
        "\n",
        "    if inputs_embeds is None:\n",
        "        inputs_embeds = self.wte(input_ids)\n",
        "    position_embeds = self.wpe(position_ids)\n",
        "    hidden_states = inputs_embeds + position_embeds\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        token_type_embeds = self.wte(token_type_ids)\n",
        "        hidden_states = hidden_states + token_type_embeds\n",
        "\n",
        "    hidden_states = self.drop(hidden_states)\n",
        "\n",
        "    output_shape = input_shape + (hidden_states.size(-1),)\n",
        "\n",
        "    presents = () if use_cache else None\n",
        "    all_self_attentions = () if output_attentions else None\n",
        "    all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "    all_hidden_states = () if output_hidden_states else None\n",
        "    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = block(\n",
        "            hidden_states,\n",
        "            layer_past=layer_past,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask[i],\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        if use_cache is True:\n",
        "            presents = presents + (outputs[1],)\n",
        "\n",
        "        if output_attentions:\n",
        "            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
        "            if self.config.add_cross_attention:\n",
        "                all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
        "\n",
        "\n",
        "    hidden_states = self.ln_f(hidden_states)\n",
        "\n",
        "    hidden_states = hidden_states.view(*output_shape)\n",
        "    # Add last hidden state\n",
        "    if output_hidden_states:\n",
        "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "    if not return_dict:\n",
        "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
        "\n",
        "    return BaseModelOutputWithPastAndCrossAttentions(\n",
        "        last_hidden_state=hidden_states,\n",
        "        past_key_values=presents,\n",
        "        hidden_states=all_hidden_states,\n",
        "        attentions=all_self_attentions,\n",
        "        cross_attentions=all_cross_attentions,\n",
        "    )\n",
        "\n",
        "\n",
        "class GPT2LMHeadModel(GPT2PreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.transformer = GPT2Model(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights from scratch\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
        "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
        "        # only last token for inputs_ids if past is defined in kwargs\n",
        "        if past:\n",
        "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
        "            if token_type_ids is not None:\n",
        "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
        "\n",
        "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
        "        position_ids = kwargs.get(\"position_ids\", None)\n",
        "\n",
        "        if attention_mask is not None and position_ids is None:\n",
        "            # create position_ids on the fly for batch generation\n",
        "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
        "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
        "            if past:\n",
        "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
        "        else:\n",
        "            position_ids = None\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"past_key_values\": past,\n",
        "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "            \"position_ids\": position_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "        }\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + transformer_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=transformer_outputs.past_key_values,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "            cross_attentions=transformer_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor) -> Tuple[Tuple[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        This function is used to re-order the :obj:`past_key_values` cache if\n",
        "        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
        "        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
        "        \"\"\"\n",
        "        return tuple(\n",
        "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
        "            for layer_past in past\n",
        "        )\n",
        "\n",
        "\n",
        "# Load Wikipedia dataset\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\",trust_remote_code=True)\n",
        "\n",
        "# Tokenize the dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Define data collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Initialize the model\n",
        "config = GPT2Config()\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=tokenized_datasets,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYtrzX3CplGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}